{"cells":[{"cell_type":"code","source":["\ncolumns = [\"language\",\"users_count\"]\ndata = [(\"Java\", \"20000\"), (\"Python\", \"100000\"), (\"Scala\", \"3000\")]\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"cfa4dee3-d184-4023-ac68-3dd6d817e85b"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["from pyspark.sql import SparkSession\n\nspark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\nrdd = spark.sparkContext.parallelize(data)\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0003c6f1-76e3-4ca4-ab97-d6dfda45c943"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["\ndfFromRDD1 = rdd.toDF()\ndfFromRDD1.printSchema()\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"02cebbe9-028a-42ec-8c76-e935194daec4"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"root\n |-- _1: string (nullable = true)\n |-- _2: string (nullable = true)\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["root\n |-- _1: string (nullable = true)\n |-- _2: string (nullable = true)\n\n"]}}],"execution_count":0},{"cell_type":"code","source":["\ncolumns = [\"language\",\"users_count\"]\ndfFromRDD1 = rdd.toDF(columns)\ndfFromRDD1.printSchema()\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"190c0b05-fc2b-47d4-a3e4-77300fea4238"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"root\n |-- language: string (nullable = true)\n |-- users_count: string (nullable = true)\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["root\n |-- language: string (nullable = true)\n |-- users_count: string (nullable = true)\n\n"]}}],"execution_count":0},{"cell_type":"code","source":["\ndfFromRDD2 = spark.createDataFrame(rdd).toDF(*columns)\n\ndfFromRDD2.printSchema()\ndfFromRDD2.display()\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"96ca2667-9f7a-4cfb-af7d-76dd4d53ea05"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"root\n |-- language: string (nullable = true)\n |-- users_count: string (nullable = true)\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["root\n |-- language: string (nullable = true)\n |-- users_count: string (nullable = true)\n\n"]}},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"overflow":false,"datasetInfos":[],"data":[["Java","20000"],["Python","100000"],["Scala","3000"]],"plotOptions":{"displayType":"table","customPlotOptions":{},"pivotColumns":null,"pivotAggregation":null,"xColumns":null,"yColumns":null},"columnCustomDisplayInfos":{},"aggType":"","isJsonSchema":true,"removedWidgets":[],"aggSchema":[],"schema":[{"name":"language","type":"\"string\"","metadata":"{}"},{"name":"users_count","type":"\"string\"","metadata":"{}"}],"aggError":"","aggData":[],"addedWidgets":{},"metadata":{},"dbfsResultPath":null,"type":"table","aggOverflow":false,"aggSeriesLimitReached":false,"arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>language</th><th>users_count</th></tr></thead><tbody><tr><td>Java</td><td>20000</td></tr><tr><td>Python</td><td>100000</td></tr><tr><td>Scala</td><td>3000</td></tr></tbody></table></div>"]}}],"execution_count":0},{"cell_type":"code","source":["\ndfFromData2 = spark.createDataFrame(data).toDF(*columns)\n\ndfFromData2.printSchema()\ndfFromData2.display()\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"81dd1c49-3a0f-49ca-99e0-3cab85d46c15"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"root\n |-- language: string (nullable = true)\n |-- users_count: string (nullable = true)\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["root\n |-- language: string (nullable = true)\n |-- users_count: string (nullable = true)\n\n"]}},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"overflow":false,"datasetInfos":[],"data":[["Java","20000"],["Python","100000"],["Scala","3000"]],"plotOptions":{"displayType":"table","customPlotOptions":{},"pivotColumns":null,"pivotAggregation":null,"xColumns":null,"yColumns":null},"columnCustomDisplayInfos":{},"aggType":"","isJsonSchema":true,"removedWidgets":[],"aggSchema":[],"schema":[{"name":"language","type":"\"string\"","metadata":"{}"},{"name":"users_count","type":"\"string\"","metadata":"{}"}],"aggError":"","aggData":[],"addedWidgets":{},"metadata":{},"dbfsResultPath":null,"type":"table","aggOverflow":false,"aggSeriesLimitReached":false,"arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>language</th><th>users_count</th></tr></thead><tbody><tr><td>Java</td><td>20000</td></tr><tr><td>Python</td><td>100000</td></tr><tr><td>Scala</td><td>3000</td></tr></tbody></table></div>"]}}],"execution_count":0},{"cell_type":"code","source":["\nrowData = map(lambda x: Row(*x), data) \ndfFromData3 = spark.createDataFrame(rowData,columns)\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a8b8f3a5-0bc8-44ef-93dc-111fad4a2205"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)\n\u001B[0;32m<command-1064759925509157>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[0mrowData\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mmap\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;32mlambda\u001B[0m \u001B[0mx\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0mRow\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0mx\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdata\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 2\u001B[0;31m \u001B[0mdfFromData3\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mspark\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcreateDataFrame\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mrowData\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0mcolumns\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\n\u001B[0;32m/databricks/spark/python/pyspark/sql/session.py\u001B[0m in \u001B[0;36mcreateDataFrame\u001B[0;34m(self, data, schema, samplingRatio, verifySchema)\u001B[0m\n\u001B[1;32m    720\u001B[0m             return super(SparkSession, self).createDataFrame(\n\u001B[1;32m    721\u001B[0m                 data, schema, samplingRatio, verifySchema)\n\u001B[0;32m--> 722\u001B[0;31m         \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_create_dataframe\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdata\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mschema\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0msamplingRatio\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mverifySchema\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    723\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    724\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0m_create_dataframe\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdata\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mschema\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0msamplingRatio\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mverifySchema\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/pyspark/sql/session.py\u001B[0m in \u001B[0;36m_create_dataframe\u001B[0;34m(self, data, schema, samplingRatio, verifySchema)\u001B[0m\n\u001B[1;32m    752\u001B[0m                 \u001B[0mrdd\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mschema\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_createFromRDD\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdata\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mmap\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mprepare\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mschema\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0msamplingRatio\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    753\u001B[0m             \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 754\u001B[0;31m                 \u001B[0mrdd\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mschema\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_createFromLocal\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mmap\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mprepare\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdata\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mschema\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    755\u001B[0m             \u001B[0mjrdd\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_jvm\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mSerDeUtil\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtoJavaArray\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mrdd\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_to_java_object_rdd\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    756\u001B[0m             \u001B[0mjdf\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_jsparkSession\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mapplySchemaToPythonRDD\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mjrdd\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mrdd\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mschema\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mjson\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/pyspark/sql/session.py\u001B[0m in \u001B[0;36m_createFromLocal\u001B[0;34m(self, data, schema)\u001B[0m\n\u001B[1;32m    528\u001B[0m         \u001B[0mwrite\u001B[0m \u001B[0mtemp\u001B[0m \u001B[0mfiles\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    529\u001B[0m         \"\"\"\n\u001B[0;32m--> 530\u001B[0;31m         \u001B[0mdata\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mschema\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_wrap_data_schema\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdata\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mschema\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    531\u001B[0m         \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_sc\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mparallelize\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdata\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mschema\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    532\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/pyspark/sql/session.py\u001B[0m in \u001B[0;36m_wrap_data_schema\u001B[0;34m(self, data, schema)\u001B[0m\n\u001B[1;32m    504\u001B[0m         \u001B[0;31m# make sure data could consumed multiple times\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    505\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0misinstance\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdata\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mlist\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 506\u001B[0;31m             \u001B[0mdata\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mlist\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdata\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    507\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    508\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mschema\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0;32mNone\u001B[0m \u001B[0;32mor\u001B[0m \u001B[0misinstance\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mschema\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m(\u001B[0m\u001B[0mlist\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtuple\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m<command-1064759925509157>\u001B[0m in \u001B[0;36m<lambda>\u001B[0;34m(x)\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0mrowData\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mmap\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;32mlambda\u001B[0m \u001B[0mx\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0mRow\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0mx\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdata\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      2\u001B[0m \u001B[0mdfFromData3\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mspark\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcreateDataFrame\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mrowData\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0mcolumns\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;31mNameError\u001B[0m: name 'Row' is not defined","errorSummary":"<span class='ansi-red-fg'>NameError</span>: name 'Row' is not defined","metadata":{},"errorTraceType":"ansi","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/plain":["\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)\n\u001B[0;32m<command-1064759925509157>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[0mrowData\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mmap\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;32mlambda\u001B[0m \u001B[0mx\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0mRow\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0mx\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdata\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 2\u001B[0;31m \u001B[0mdfFromData3\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mspark\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcreateDataFrame\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mrowData\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0mcolumns\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\n\u001B[0;32m/databricks/spark/python/pyspark/sql/session.py\u001B[0m in \u001B[0;36mcreateDataFrame\u001B[0;34m(self, data, schema, samplingRatio, verifySchema)\u001B[0m\n\u001B[1;32m    720\u001B[0m             return super(SparkSession, self).createDataFrame(\n\u001B[1;32m    721\u001B[0m                 data, schema, samplingRatio, verifySchema)\n\u001B[0;32m--> 722\u001B[0;31m         \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_create_dataframe\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdata\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mschema\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0msamplingRatio\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mverifySchema\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    723\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    724\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0m_create_dataframe\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdata\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mschema\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0msamplingRatio\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mverifySchema\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/pyspark/sql/session.py\u001B[0m in \u001B[0;36m_create_dataframe\u001B[0;34m(self, data, schema, samplingRatio, verifySchema)\u001B[0m\n\u001B[1;32m    752\u001B[0m                 \u001B[0mrdd\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mschema\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_createFromRDD\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdata\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mmap\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mprepare\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mschema\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0msamplingRatio\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    753\u001B[0m             \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 754\u001B[0;31m                 \u001B[0mrdd\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mschema\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_createFromLocal\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mmap\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mprepare\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdata\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mschema\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    755\u001B[0m             \u001B[0mjrdd\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_jvm\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mSerDeUtil\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtoJavaArray\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mrdd\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_to_java_object_rdd\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    756\u001B[0m             \u001B[0mjdf\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_jsparkSession\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mapplySchemaToPythonRDD\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mjrdd\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mrdd\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mschema\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mjson\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/pyspark/sql/session.py\u001B[0m in \u001B[0;36m_createFromLocal\u001B[0;34m(self, data, schema)\u001B[0m\n\u001B[1;32m    528\u001B[0m         \u001B[0mwrite\u001B[0m \u001B[0mtemp\u001B[0m \u001B[0mfiles\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    529\u001B[0m         \"\"\"\n\u001B[0;32m--> 530\u001B[0;31m         \u001B[0mdata\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mschema\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_wrap_data_schema\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdata\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mschema\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    531\u001B[0m         \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_sc\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mparallelize\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdata\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mschema\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    532\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/pyspark/sql/session.py\u001B[0m in \u001B[0;36m_wrap_data_schema\u001B[0;34m(self, data, schema)\u001B[0m\n\u001B[1;32m    504\u001B[0m         \u001B[0;31m# make sure data could consumed multiple times\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    505\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0misinstance\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdata\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mlist\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 506\u001B[0;31m             \u001B[0mdata\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mlist\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdata\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    507\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    508\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mschema\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0;32mNone\u001B[0m \u001B[0;32mor\u001B[0m \u001B[0misinstance\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mschema\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m(\u001B[0m\u001B[0mlist\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtuple\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m<command-1064759925509157>\u001B[0m in \u001B[0;36m<lambda>\u001B[0;34m(x)\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0mrowData\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mmap\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;32mlambda\u001B[0m \u001B[0mx\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0mRow\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0mx\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdata\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      2\u001B[0m \u001B[0mdfFromData3\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mspark\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcreateDataFrame\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mrowData\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0mcolumns\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;31mNameError\u001B[0m: name 'Row' is not defined"]}}],"execution_count":0},{"cell_type":"code","source":["\nfrom pyspark.sql.types import StructType,StructField, StringType, IntegerType\ndata2 = [(\"James\",\"\",\"Smith\",\"36636\",\"M\",3000),\n    (\"Michael\",\"Rose\",\"\",\"40288\",\"M\",4000),\n    (\"Robert\",\"\",\"Williams\",\"42114\",\"M\",4000),\n    (\"Maria\",\"Anne\",\"Jones\",\"39192\",\"F\",4000),\n    (\"Jen\",\"Mary\",\"Brown\",\"\",\"F\",-1)\n  ]\n\nschema = StructType([ \\\n    StructField(\"firstname\",StringType(),True), \n    StructField(\"middlename\",StringType(),True), \\\n    StructField(\"lastname\",StringType(),True), \\\n    StructField(\"id\", StringType(), True), \\\n    StructField(\"gender\", StringType(), True), \\\n    StructField(\"salary\", IntegerType(), True) \\\n  ])\n \ndf = spark.createDataFrame(data=data2,schema=schema)\ndf.printSchema()\ndf.show(truncate=False)\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ac49b6d8-963b-4fc2-a3fc-73435b601762"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"root\n |-- firstname: string (nullable = true)\n |-- middlename: string (nullable = true)\n |-- lastname: string (nullable = true)\n |-- id: string (nullable = true)\n |-- gender: string (nullable = true)\n |-- salary: integer (nullable = true)\n\n+---------+----------+--------+-----+------+------+\n|firstname|middlename|lastname|id   |gender|salary|\n+---------+----------+--------+-----+------+------+\n|James    |          |Smith   |36636|M     |3000  |\n|Michael  |Rose      |        |40288|M     |4000  |\n|Robert   |          |Williams|42114|M     |4000  |\n|Maria    |Anne      |Jones   |39192|F     |4000  |\n|Jen      |Mary      |Brown   |     |F     |-1    |\n+---------+----------+--------+-----+------+------+\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["root\n |-- firstname: string (nullable = true)\n |-- middlename: string (nullable = true)\n |-- lastname: string (nullable = true)\n |-- id: string (nullable = true)\n |-- gender: string (nullable = true)\n |-- salary: integer (nullable = true)\n\n+---------+----------+--------+-----+------+------+\n|firstname|middlename|lastname|id   |gender|salary|\n+---------+----------+--------+-----+------+------+\n|James    |          |Smith   |36636|M     |3000  |\n|Michael  |Rose      |        |40288|M     |4000  |\n|Robert   |          |Williams|42114|M     |4000  |\n|Maria    |Anne      |Jones   |39192|F     |4000  |\n|Jen      |Mary      |Brown   |     |F     |-1    |\n+---------+----------+--------+-----+------+------+\n\n"]}}],"execution_count":0},{"cell_type":"code","source":[""],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9cba96f0-dae0-4d07-a630-355d490a48bf"}},"outputs":[],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"PySpark â€“ Create DataFrame","dashboards":[],"notebookMetadata":{"pythonIndentUnit":4},"language":"python","widgets":{},"notebookOrigID":1064759925509150}},"nbformat":4,"nbformat_minor":0}
