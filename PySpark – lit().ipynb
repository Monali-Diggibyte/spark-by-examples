{"cells":[{"cell_type":"code","source":["\"\"\"\nPySpark SQL functions lit() and typedLit() are used to add a new column to DataFrame by assigning a literal or constant value. Both these functions return Column type as return type.\n\"\"\"\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"def76c8a-e4d9-44b1-a767-03fe72bd8ece"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["\nimport pyspark\nfrom pyspark.sql import SparkSession\n\nspark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\ndata = [(\"111\",50000),(\"222\",60000),(\"333\",40000)]\ncolumns= [\"EmpId\",\"Salary\"]\ndf = spark.createDataFrame(data = data, schema = columns)\ndf.show(truncate= False)\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3978730e-cbbe-4d70-93c0-78ea482c7fba"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"+-----+------+\n|EmpId|Salary|\n+-----+------+\n|111  |50000 |\n|222  |60000 |\n|333  |40000 |\n+-----+------+\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+-----+------+\n|EmpId|Salary|\n+-----+------+\n|111  |50000 |\n|222  |60000 |\n|333  |40000 |\n+-----+------+\n\n"]}}],"execution_count":0},{"cell_type":"code","source":["\"\"\"\nPySpark lit() function is used to add constant or literal value as a new column to the DataFrame.\n\"\"\"\n\n\nfrom pyspark.sql.functions import col,lit\n\ndf2 = df.select(col(\"EmpId\"),col(\"Salary\"),lit(\"1\").alias(\"lit_value1\"))\n\ndf2.show(truncate=False)\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9facfb15-01e9-4aff-a834-39ec83a382d3"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"+-----+------+----------+\n|EmpId|Salary|lit_value1|\n+-----+------+----------+\n|111  |50000 |1         |\n|222  |60000 |1         |\n|333  |40000 |1         |\n+-----+------+----------+\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+-----+------+----------+\n|EmpId|Salary|lit_value1|\n+-----+------+----------+\n|111  |50000 |1         |\n|222  |60000 |1         |\n|333  |40000 |1         |\n+-----+------+----------+\n\n"]}}],"execution_count":0},{"cell_type":"code","source":["\"\"\"  lit() function with withColumn \"\"\"\n\n\nfrom pyspark.sql.functions import when, lit, col\n\n#df3 = df2.withColumn(\"lit_value2\", when((col(\"Salary\") >=40000 & col(\"Salary\") <= 50000),lit(\"100\")).otherwise(lit(\"200\")))\n\n\ndf3 = df2.withColumn(\"lit_value2\",when((col(\"Salary\")>=40000 ), lit(\"100\")).otherwise(lit(\"200\")))\n\ndf3.show(truncate=False)\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2e546195-28ef-4be1-a159-562d557e20aa"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"+-----+------+----------+----------+\n|EmpId|Salary|lit_value1|lit_value2|\n+-----+------+----------+----------+\n|111  |50000 |1         |100       |\n|222  |60000 |1         |100       |\n|333  |40000 |1         |100       |\n+-----+------+----------+----------+\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+-----+------+----------+----------+\n|EmpId|Salary|lit_value1|lit_value2|\n+-----+------+----------+----------+\n|111  |50000 |1         |100       |\n|222  |60000 |1         |100       |\n|333  |40000 |1         |100       |\n+-----+------+----------+----------+\n\n"]}}],"execution_count":0},{"cell_type":"code","source":["\nimport pyspark\nfrom pyspark.sql import SparkSession\n\nspark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\n\ndata = [(\"111\",50000),(\"222\",60000),(\"333\",40000)]\ncolumns= [\"EmpId\",\"Salary\"]\ndf = spark.createDataFrame(data = data, schema = columns)\n\ndf.printSchema()\ndf.show(truncate=False)\n\nfrom pyspark.sql.functions import col,lit\ndf2 = df.select(col(\"EmpId\"),col(\"Salary\"),lit(\"1\").alias(\"lit_value1\"))\ndf2.show(truncate=False)\n\nfrom pyspark.sql.functions import when\ndf3 = df2.withColumn(\"lit_value2\", when(col(\"Salary\") >=40000 ,lit(\"100\")).otherwise(lit(\"200\")))\ndf3.show(truncate=False)\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e1db201f-5d73-4d96-b452-55e8388f2b86"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"root\n |-- EmpId: string (nullable = true)\n |-- Salary: long (nullable = true)\n\n+-----+------+\n|EmpId|Salary|\n+-----+------+\n|111  |50000 |\n|222  |60000 |\n|333  |40000 |\n+-----+------+\n\n+-----+------+----------+\n|EmpId|Salary|lit_value1|\n+-----+------+----------+\n|111  |50000 |1         |\n|222  |60000 |1         |\n|333  |40000 |1         |\n+-----+------+----------+\n\n+-----+------+----------+----------+\n|EmpId|Salary|lit_value1|lit_value2|\n+-----+------+----------+----------+\n|111  |50000 |1         |100       |\n|222  |60000 |1         |100       |\n|333  |40000 |1         |100       |\n+-----+------+----------+----------+\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["root\n |-- EmpId: string (nullable = true)\n |-- Salary: long (nullable = true)\n\n+-----+------+\n|EmpId|Salary|\n+-----+------+\n|111  |50000 |\n|222  |60000 |\n|333  |40000 |\n+-----+------+\n\n+-----+------+----------+\n|EmpId|Salary|lit_value1|\n+-----+------+----------+\n|111  |50000 |1         |\n|222  |60000 |1         |\n|333  |40000 |1         |\n+-----+------+----------+\n\n+-----+------+----------+----------+\n|EmpId|Salary|lit_value1|lit_value2|\n+-----+------+----------+----------+\n|111  |50000 |1         |100       |\n|222  |60000 |1         |100       |\n|333  |40000 |1         |100       |\n+-----+------+----------+----------+\n\n"]}}],"execution_count":0},{"cell_type":"code","source":[""],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8d5cc627-c33c-45f4-837c-d71d9128b834"}},"outputs":[],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"PySpark â€“ lit()","dashboards":[],"notebookMetadata":{"pythonIndentUnit":4},"language":"python","widgets":{},"notebookOrigID":4448613729404005}},"nbformat":4,"nbformat_minor":0}
